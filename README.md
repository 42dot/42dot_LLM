# ì±—ë² ì´ì»¤ (ChatBaker)

ì±—ë² ì´ì»¤ (ChatBaker)ëŠ” 42dotì—ì„œ ê°œë°œí•œ ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.
- ëŒ€í•œë¯¼êµ­ ìµœì´ˆì˜ í•œì˜í†µí•© ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ (Large Language Model, LLM) í•™ìŠµ
- í•œì˜í†µí•© PLMì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ í•™ìŠµ
- ìì²´ êµ¬ì¶•í•œ (ìˆ˜ì§‘, ì •ì œ) ë°ì´í„°, í•™ìŠµ ì¸í”„ë¼ ì‚¬ìš©

### [ì˜¨ë¼ì¸ ë°ëª¨](demolink)
ChatBaker í•œì˜í†µí•© 7B ëª¨ë¸ì„ ê²½í—˜í•´ë³´ì„¸ìš”!

[ë°ëª¨ ìƒ˜í”Œ GIF ì¶”ê°€]


## ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸
ì±—ë² ì´ì»¤ (ChatBaker) í•™ìŠµì— [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)ì˜ ë² ì´ìŠ¤ ì½”ë“œì¸ [FastChat](https://github.com/lm-sys/FastChat)ì„ ì‚¬ìš©í–ˆê³ , ì‚¬ìš©í•œ íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

| Hyperparameter | Global Batch Size | Learning rate | Epochs | Max length | Weight decay | Warmup ratio |
| -- | -- | -- | -- | -- | -- | -- |
| ChatBaker | 16 | 2e-5 | 3/6/9 | 2,048 | 0 | 0.03 |

A100 80G GPU 8ì¥ì„ í•™ìŠµì— ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

| Model | ChatBaker-1.3B-kr | ChatBaker-1.3B-kr-en | ChatBaker-7B-kr-en |
| -- | -- | -- | -- |
| Training time | 9 hours | 20 hours | 48 hours |

### í•™ìŠµ ë°ì´í„°ì…‹

ìš”ì²­ ë° ì´ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ì´ë£¨ì–´ì§„ ëŒ€í™”í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
- í•œêµ­ì–´: ì•½ 15ë§Œ ê±´
- ì˜ì–´: ì•½ 25ë§Œ ê±´

ì£¼) ì±—ë² ì´ì»¤ (ChatBaker) í•™ìŠµì— ì‚¬ìš©í•œ ë°ì´í„°ëŠ” ê³µê°œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹ , ë‹¤ì–‘í•œ í•œêµ­ì–´ ([evolve-instruct](https://github.com/lcw99/evolve-instruct), [ko-lima-vicuna](https://huggingface.co/datasets/changpt/ko-lima-vicuna), ë“±) ë° ì˜ì–´ (ShareGPT, OpenAssistant, etc.) ëŒ€í™” ë°ì´í„°ê°€ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### í‰ê°€
- ë¹„êµëŒ€ìƒ:
  - [Polyglot-Ko-1.3B-SFT]: [Polyglot-Ko-1.3B](https://huggingface.co/EleutherAI/polyglot-ko-1.3b) ëª¨ë¸ì— ChatBakerì™€ ë™ì¼í•œ ë°ì´í„°ë¡œ í•™ìŠµí•œ ëª¨ë¸
  - [ChatGPT](https://chat.openai.com/): OpenAIê°€ ê³µê°œí•œ ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ ì„œë¹„ìŠ¤
  - [Bard](https://bard.google.com/): Googleì´ ê³µê°œí•œ ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ ì„œë¹„ìŠ¤
- í‰ê°€ ë°ì´í„°ì…‹:
[ë°ì´í„°ì…‹ ë‚´ìš© ì¶”ê°€]
- í‰ê°€ ë°©ë²•:
[í‰ê°€ ë°©ë²• ì¶”ê°€]


<img src="asset/image.png" width="90%" height="90%"/>

[ì„±ëŠ¥ ê·¸ë˜í”„ ëŒ€ì²´]

#### ìš”ì•½
[í‰ê°€ ê²°ê³¼ ì¶”ê°€]


## ì‚¬ì „ í•™ìŠµ ëª¨ë¸ (PLM)
### ì•„í‚¤í…ì³
Transformer decoder ê¸°ë°˜ì˜ [LLaMA](https://arxiv.org/abs/2302.13971) ì•„í‚¤í…ì³ë¥¼ ì‚¬ìš©í–ˆê³ , ì‚¬ìš©í•œ íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

| Hyperparameter | Global Batch Size | Learning rate | Epochs | Max length | Weight decay |
| -- | -- | -- | -- | -- | -- |
| PLM | | | | 2,048 | |

| Hyperparameter | Layers | Attention heads | Hidden size |  |  |
| -- | -- | -- | -- | -- | -- |
| 1.3B | 24 | 32 | 2,048 | | |
| 7B | | | | | |

A100 80G GPU 256ì¥ (8 GPUs * 32 Nodes)ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

| Model | KO 1.3B | KOEN 1.3B | KOEN 7B |
| -- | -- | -- | -- |
| Training time | xx days | xx days | 30 days+ |


### í•™ìŠµ ë°ì´í„°ì…‹
- í•œêµ­ì–´: 100B í† í°
- ì˜ì–´: 1T í† í°

### í† í¬ë‚˜ì´ì €
Byte-level BPE í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í–ˆê³ , í•œêµ­ì–´ì™€ í•œì˜í†µí•© í† í¬ë‚˜ì´ì €ëŠ” ê°ê° ë¬¸ì„œ 100ë§Œê±´ìœ¼ë¡œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### í‰ê°€
#### í•œêµ­ì–´
- ë¹„êµëŒ€ìƒ:
  - [Polyglot-Ko](https://github.com/EleutherAI/polyglot): LLaMA ì•„í‚¤í…Œì³ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ 213B í† í° (863 GB)ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸
  - [KoGPT2](https://github.com/SKT-AI/KoGPT2): GPT ì•„í‚¤í…ì³ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 40GB ì´ìƒì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸
- í‰ê°€ ë°ì´í„°ì…‹:
  - [KoBEST](https://huggingface.co/datasets/skt/kobest_v1) 
  - HyperClovaì—ì„œ í‰ê°€í•œ ë°ì´í„°ì…‹ì€?
- ì§€í‘œ: Macro F1

[ì„±ëŠ¥ ê·¸ë˜í”„ ì¶”ê°€]

#### ì˜ì–´
- ë¹„êµëŒ€ìƒ:
  - [OPT](OPT ë§í¬): 
  - [MPT](MPT ë§í¬): 
  - [LLaMA](LLaMA ë§í¬): 
- í‰ê°€ ë°ì´í„°ì…‹: ì˜ì–´ Benchmarks 14ì¢…
    - anli, arc, boolq, hellaswag, openbookqa, piqa, record, rte, truthfulqa_mc, wic, winogrande

[ì„±ëŠ¥ ê·¸ë˜í”„ ì¶”ê°€]

#### ìš”ì•½
- í•œêµ­ì–´ ëª¨ë¸ì€ polyglot-ko 1.3B ëŒ€ë¹„ ì•½ 5% ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
- í•œì˜í†µí•© ëª¨ë¸ì€ kogpt (skt) ëŒ€ë¹„ ì•½ 2% ë†’ì€ ì„±ëŠ¥ê³¼, polyglot-ko 1.3B ëŒ€ë¹„ ì˜¤ì°¨ë²”ìœ„ ì´ë‚´ì˜ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

### ëª¨ë¸ ê³µê°œ

ğŸ¤—[í•œì˜í†µí•©-1.3B](í—ˆê¹…í˜ì´ìŠ¤ ë§í¬)


## í•œê³„ì 
ë‹¤ë¥¸ LLM (ChatGPT, Vicuna, ë“±)ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì±—ë² ì´ì»¤ (ChatBaker) ë˜í•œ ë§ì€ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. 

We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI moderation API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

## ë¼ì´ì„¼ìŠ¤
- ì½”ë“œ: 
- ëª¨ë¸:


